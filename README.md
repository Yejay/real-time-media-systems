# Whisper STT Proof of Concept

Ein einfaches Python-Script zur automatischen Untertitel-Generierung aus Videos mit OpenAI Whisper.

## üéØ Projekt√ºbersicht

Dieses Mini-MVP konvertiert Video-Dateien automatisch zu SRT-Untertitel-Dateien mit Speech-to-Text Technologie.

**Projekt:** Real-Time Media Systems  
**Team:** Yejay Demirkan, Marcus Schumann, Vasiliki Ioannidou  
**Semester:** SoSe 2025

### Features
- ‚úÖ Video zu Audio Konvertierung (ffmpeg)
- ‚úÖ Speech-to-Text mit OpenAI Whisper
- ‚úÖ SRT-Untertitel Generation
- ‚úÖ Command-Line Interface
- ‚úÖ Keyword-Extraktion (optional)

## üöÄ Quick Start

### 1. Installation

```bash
# Python Environment erstellen
python -m venv whisper-env
source whisper-env/bin/activate  # macOS/Linux

# Dependencies installieren
pip install -r requirements.txt

# ffmpeg installieren (macOS mit Homebrew)
brew install ffmpeg
```

### 2. Erste Verwendung

```bash
# Video zu SRT konvertieren
python main.py test_videos/your_video.mp4

# Output wird gespeichert in:
# output/your_video.srt
```

## üìÅ Projektstruktur

```
whisper-stt-poc/
‚îú‚îÄ‚îÄ main.py              # Entry point
‚îú‚îÄ‚îÄ audio_extractor.py   # Video ‚Üí Audio
‚îú‚îÄ‚îÄ srt_generator.py     # Whisper ‚Üí SRT
‚îú‚îÄ‚îÄ keyword_extractor.py # Optional keyword extraction
‚îú‚îÄ‚îÄ requirements.txt     # Dependencies
‚îú‚îÄ‚îÄ README.md           # This file
‚îú‚îÄ‚îÄ test_videos/        # Test files (add your videos here)
‚îî‚îÄ‚îÄ output/             # Generated SRTs
```

## üîß Usage

### Basic Usage
```bash
python main.py input_video.mp4
```

### Unterst√ºtzte Formate
- **Input:** MP4, AVI, MOV, MKV (alle ffmpeg-kompatiblen Formate)
- **Output:** SRT-Untertitel-Dateien

## üß™ Testing

### Test-Videos hinzuf√ºgen
1. Videos in `test_videos/` Ordner kopieren
2. Script ausf√ºhren: `python main.py test_videos/my_video.mp4`
3. SRT-Datei in `output/` √ºberpr√ºfen

### Testing Checklist
- [ ] SRT-File √∂ffnet sich in Video-Player (VLC)
- [ ] Untertitel sind grob synchron (¬±2 Sekunden OK)
- [ ] Deutsche Umlaute werden korrekt dargestellt
- [ ] Keine leeren Untertitel-Segmente

## ‚öôÔ∏è Konfiguration

### Whisper Model-Gr√∂√üen
Im `srt_generator.py` kann das Whisper-Model angepasst werden:

```python
# Verf√ºgbare Models (Geschwindigkeit vs. Genauigkeit):
model = whisper.load_model("tiny")    # Schnellstes
model = whisper.load_model("base")    # Standard (empfohlen)
model = whisper.load_model("small")   # Bessere Qualit√§t
model = whisper.load_model("medium")  # Noch besser
model = whisper.load_model("large")   # Beste Qualit√§t, langsam
```

### Sprache
Standardm√§√üig auf Deutsch eingestellt. In `srt_generator.py` √§ndern:
```python
result = model.transcribe(audio_path, language="de")  # Deutsch
result = model.transcribe(audio_path, language="en")  # Englisch
result = model.transcribe(audio_path, language=None)  # Auto-detect
```

## üêõ Troubleshooting

### H√§ufige Probleme

**ffmpeg nicht gefunden:**
```bash
# macOS
brew install ffmpeg

# Ubuntu/Debian
sudo apt install ffmpeg

# Windows
# Download von https://ffmpeg.org/
```

**Whisper Model Download-Fehler:**
```bash
# Model manuell herunterladen
python -c "import whisper; whisper.load_model('base')"
```

**Encoding-Probleme bei deutschen Umlauten:**
- SRT-Files werden mit UTF-8 encoding gespeichert
- Bei Problemen Video-Player Encoding auf UTF-8 stellen

## üìà Performance

### Typische Verarbeitungszeiten (MacBook Pro M1):
- **2-3 Min Video:** ~30 Sekunden
- **10-15 Min Video:** ~2-3 Minuten  
- **1 Stunde Video:** ~8-12 Minuten

### Optimization-Tipps:
- Nutze `tiny` model f√ºr schnelle Tests
- Nutze `base` model f√ºr Production
- GPU-Acceleration m√∂glich mit CUDA (Linux/Windows)

## üîÆ Roadmap

### Aktueller Sprint (Tag 1-7):
- [x] Basic Video‚ÜíSRT Pipeline
- [x] Command-Line Interface
- [ ] Testing mit verschiedenen Video-L√§ngen
- [ ] Keyword-Extraktion (optional)
- [ ] README und Dokumentation

### M√∂gliche Erweiterungen:
- Web-Interface mit Flask/Streamlit
- Batch-Processing f√ºr mehrere Videos
- API-Endpoint f√ºr Integration
- Database f√ºr Metadaten
- Erweiterte Keyword-Analyse

## ü§ù Contributing

Dies ist ein Proof-of-Concept f√ºr ein Universit√§tsprojekt. 

### Development Setup:
```bash
git clone <repository>
cd real-time-media-systems
python -m venv whisper-env
source whisper-env/bin/activate
pip install -r requirements.txt
```  
**Ziel:** Automatische Untertitel-Generierung und Keyword-Extraktion f√ºr BHT-Videoplattform

---

## 1. Problemstellung und Zielsetzung

### Hauptziel
- Barrierefreie Videos durch automatische Untertitel-Generierung
- Verbesserung der Zug√§nglichkeit von Vorlesungsvideos
- Automatische Kapitel-Timestamps durch Keyword-Extraktion (Nice-to-have)

### Herausforderungen
- Kein direkter Zugriff auf Opencast/HRZ-System
- Deutsche Sprache mit Fachvokabular
- Verschiedene Dialekte und Sprechgeschwindigkeiten
- Integration in bestehende Plattformen (Moodle, BHT-Videoplattform)

---

## 2. Technologie-Vergleich: Speech-to-Text-Systeme

### Cloud-basierte L√∂sungen

#### Google Cloud Speech-to-Text
- **Vorteile:**
  - Exzellente Deutsch-Unterst√ºtzung
  - Echtzeit-Verarbeitung m√∂glich
  - Automatische Punktuation und Diarisierung
  - Hohe Genauigkeit bei Fachsprache
- **Nachteile:**
  - Kosten bei gr√∂√üeren Mengen
  - Datenschutz-Bedenken (Cloud-Verarbeitung)
  - Internetverbindung erforderlich
- **Kosten:** ~$0.006 pro 15-Sekunden-Segment
- **API-Integration:** Sehr gut dokumentiert

#### Microsoft Azure Speech Services
- **Vorteile:**
  - Integrierte Keyword-Extraktion
  - Gute Deutsch-Unterst√ºtzung
  - Custom Speech Models m√∂glich
  - Batch-Verarbeitung
- **Nachteile:**
  - Mittlere Preisklasse
  - Datenschutz-Aspekte
- **Kosten:** ~$1 pro Stunde Audio
- **Besonderheit:** Direkte Integration mit anderen Azure-Services

#### IBM Watson Speech to Text
- **Vorteile:**
  - Trainierbare Custom Models
  - Custom Dictionary f√ºr Fachbegriffe
  - Gute Enterprise-Integration
- **Nachteile:**
  - H√∂here Kosten
  - Komplexere API
- **Kosten:** ~$0.02 pro Minute

### Open-Source/Lokale L√∂sungen

#### OpenAI Whisper ‚≠ê **EMPFEHLUNG**
- **Vorteile:**
  - Kostenlos und Open Source
  - Hervorragende Qualit√§t f√ºr Deutsche Sprache
  - L√§uft lokal (Datenschutz)
  - Verschiedene Model-Gr√∂√üen verf√ºgbar
  - Aktive Community und Updates
- **Nachteile:**
  - Ben√∂tigt leistungsstarke Hardware f√ºr gro√üe Models
  - Keine Echtzeit-Verarbeitung
- **Models:** tiny, base, small, medium, large
- **Hardware-Anforderungen:** GPU empfohlen f√ºr large model

#### Vosk
- **Vorteile:**
  - Vollst√§ndig offline
  - Geringe Hardware-Anforderungen
  - Gute Python-Integration
- **Nachteile:**
  - Niedrigere Genauigkeit als Whisper
  - Begrenzte Deutsch-Models

---

## 3. Keyword-Extraktion: Technologien und Ans√§tze

### Regelbasierte Ans√§tze

#### spaCy + Named Entity Recognition
- **Vorteile:**
  - Lokale Verarbeitung
  - Gute Deutsch-Unterst√ºtzung
  - Erkennung von Fachbegrffen und Eigennamen
  - Team-Erfahrung vorhanden ‚≠ê
- **Anwendung:** Extraktion von Personen, Organisationen, Fachbegriffen

#### YAKE (Yet Another Keyword Extractor)
- **Vorteile:**
  - Keine Trainingsdaten erforderlich
  - Statistische Methode
  - Gute Performance bei Fachtexten
- **Nachteile:**
  - Weniger kontextbezogen

### ML-basierte Ans√§tze

#### KeyBERT ‚≠ê **EMPFEHLUNG**
- **Vorteile:**
  - BERT-basiert, sehr gute Ergebnisse
  - Kontextbezogene Keyword-Extraktion
  - Einfache Integration mit STT-Output
  - Deutsche BERT-Models verf√ºgbar
- **Anwendung:** Semantische √Ñhnlichkeit zwischen Dokument und Keywords

#### Cloud-basierte NLP-Services
- **Google Cloud Natural Language API**
- **Azure Text Analytics**
- **IBM Watson Natural Language Understanding**

---

## 4. Empfohlene Technologie-Stack

### Basis-Konfiguration (MVP)
```
Audio-Extraktion: ffmpeg
Speech-to-Text: OpenAI Whisper (medium model)
Keyword-Extraktion: KeyBERT + spaCy
Untertitel-Format: SRT/VTT
Backend: Python (Flask/FastAPI)
Frontend: React/HTML5 + JavaScript
```

### Erweiterte Konfiguration
- **STT-Alternativen:** Google Cloud Speech (f√ºr bessere Qualit√§t)
- **Preprocessing:** Rauschunterdr√ºckung mit librosa
- **Postprocessing:** Automatic Punctuation Restoration
- **Database:** PostgreSQL f√ºr Metadaten und Ergebnisse

---

## 5. Architektur und Workflow

### Verarbeitungspipeline

```mermaid
graph TD
    A[Video Upload] --> B[Audio-Extraktion]
    B --> C[Audio-Preprocessing]
    C --> D[Speech-to-Text]
    D --> E[Text-Postprocessing]
    E --> F[Keyword-Extraktion]
    F --> G[Timestamp-Zuordnung]
    G --> H[Untertitel-Generierung]
    H --> I[Export SRT/VTT]
    
    D --> J[Transkript speichern]
    F --> K[Keywords speichern]
    G --> L[Kapitel-Timestamps]
```

### Detaillierter Workflow

#### Phase 1: Audio-Verarbeitung
1. **Video-Input:** MP4, AVI, MOV, etc.
2. **Audio-Extraktion:** ffmpeg ‚Üí WAV/MP3 (16kHz, mono)
3. **Audio-Preprocessing:** 
   - Rauschunterdr√ºckung
   - Lautst√§rke-Normalisierung
   - Stille-Erkennung f√ºr bessere Segmentierung

#### Phase 2: Speech-to-Text
1. **Whisper-Verarbeitung:**
   ```python
   import whisper
   model = whisper.load_model("medium")
   result = model.transcribe("audio.wav", language="de")
   ```
2. **Output:** Transkript mit Timestamps
3. **Postprocessing:** Rechtschreibkorrekturen, Fachbegriff-Glossar

#### Phase 3: Keyword-Extraktion
1. **KeyBERT-Verarbeitung:**
   ```python
   from keybert import KeyBERT
   kw_model = KeyBERT('distilbert-base-multilingual-cased')
   keywords = kw_model.extract_keywords(transcript, 
                                       keyphrase_ngram_range=(1, 3),
                                       stop_words='german')
   ```
2. **spaCy NER:** Erg√§nzung um Eigennamen und Fachbegriffe
3. **Timestamp-Zuordnung:** Keywords zu Videozeiten zuordnen

#### Phase 4: Output-Generierung
1. **SRT-Generierung:** Standard-Untertitelformat
2. **VTT-Generierung:** F√ºr HTML5-Player
3. **Keyword-Kapitel:** JSON mit Timestamps und Beschreibungen

---

## 6. Implementierungsplan (MVP)

### Sprint 1: Basis-Pipeline (2 Wochen)
- [ ] Setup Python-Environment
- [ ] Audio-Extraktion mit ffmpeg implementieren
- [ ] Whisper-Integration und erste Tests
- [ ] Basis-SRT-Export

### Sprint 2: Keyword-Extraktion (2 Wochen)
- [ ] KeyBERT-Integration
- [ ] spaCy-Pipeline f√ºr deutsche Texte
- [ ] Keyword-Timestamp-Zuordnung
- [ ] JSON-Export f√ºr Keywords

### Sprint 3: Web-Interface (2 Wochen)
- [ ] Flask/FastAPI Backend
- [ ] File-Upload Interface
- [ ] Verarbeitungs-Status und Progress-Bar
- [ ] Download-Funktionalit√§t f√ºr SRT/Keywords

### Sprint 4: Optimierung und Tests (1 Woche)
- [ ] Batch-Processing f√ºr mehrere Videos
- [ ] Error-Handling und Logging
- [ ] Performance-Optimierungen
- [ ] Tests mit verschiedenen Videoformaten

---

## 7. Proof of Concept - Teststrategie

### Testdaten
- **3-4 Vorlesungsvideos** aus verschiedenen Fachbereichen:
  - Informatik (technische Begriffe)
  - Rechtswissenschaften (juristische Fachsprache)
  - Wirtschaft (BWL-Terminologie)
  - Ein Video mit mehreren Sprechern

### Evaluationkriterien

#### Speech-to-Text Qualit√§t
- Word Error Rate (WER)
- Korrekte Erkennung von Fachbegriffen
- Punctuation Accuracy
- Speaker Diarization (bei mehreren Sprechern)

#### Keyword-Extraktion Qualit√§t
- Relevanz der extrahierten Keywords
- Vollst√§ndigkeit (wurden wichtige Begriffe erfasst?)
- Pr√§zision (sind die Keywords tats√§chlich relevant?)
- Duplikat-Erkennung

### Bewertungsmatrix
| Kriterium | Gewichtung | Google Cloud | Whisper | Azure | Vosk |
|-----------|------------|--------------|---------|-------|------|
| Genauigkeit | 30% | | | | |
| Deutsch-Support | 25% | | | | |
| Kosten | 20% | | | | |
| Datenschutz | 15% | | | | |
| Integration | 10% | | | | |

---

## 8. Alternative Ans√§tze und Backup-Pl√§ne

### Plan B: Hybrid-Ansatz
- **STT:** Whisper f√ºr Basis-Transkription
- **Verbesserung:** Google Cloud f√ºr kritische/schwierige Passagen
- **Keyword:** Kombination aus KeyBERT und manueller Nachbearbeitung

### Plan C: Community-basiert
- **Crowdsourcing:** Studierende k√∂nnen Transkripte korrigieren
- **Gamification:** Punkte f√ºr Korrekturen
- **Quality-Control:** Mehrfach-Validierung

---

## 9. Technische Anforderungen und Setup

### Hardware-Anforderungen (Minimum)
- **CPU:** 4+ Cores
- **RAM:** 8GB (16GB empfohlen f√ºr Whisper large)
- **GPU:** Optional, aber empfohlen f√ºr gro√üe Whisper-Models
- **Storage:** SSD f√ºr bessere I/O-Performance

### Software-Dependencies
```python
# requirements.txt
whisper-openai==20240930
keybert==0.8.5
spacy==3.7.5
flask==3.0.3
ffmpeg-python==0.2.0
librosa==0.10.2
pysrt==1.1.2
pandas==2.2.2
numpy==1.26.4
```

### Docker-Setup (Optional)
```dockerfile
FROM python:3.11-slim
RUN apt-get update && apt-get install -y ffmpeg
COPY requirements.txt .
RUN pip install -r requirements.txt
# ... weitere Setup-Schritte
```

---

## 10. N√§chste Schritte

### Kurzfristig (n√§chste 2 Wochen)
1. **Setup der Entwicklungsumgebung**
2. **Erste Whisper-Tests** mit Beispielvideos
3. **KeyBERT Proof-of-Concept** 
4. **Evaluierung der Ergebnisqualit√§t**

### Mittelfristig (1 Monat)
1. **MVP-Entwicklung** mit Web-Interface
2. **Batch-Processing** f√ºr mehrere Videos
3. **Integration verschiedener STT-Services**
4. **Umfangreiche Tests** mit Uni-Videos

### Langfristig (Semesterend)
1. **Produktive Deployment-Strategie**
2. **Integration in Moodle/BHT-Plattform**
3. **Dokumentation und √úbergabe**
4. **Wartung und Updates**

---

## 11. Risiken und Mitigation

### Technische Risiken
- **Schlechte Audio-Qualit√§t:** Preprocessing mit Rauschunterdr√ºckung
- **Fachvokabular nicht erkannt:** Custom Dictionary, Nachtraining
- **Performance-Probleme:** Cloud-Services als Fallback

### Projektrisiken
- **Zeitengp√§sse:** Agiles Vorgehen, MVP-Fokus
- **Unzureichende Qualit√§t:** Mehrere STT-Services parallel testen
- **Datenschutz-Bedenken:** Lokale Verarbeitung bevorzugen

---

## 12. Ressourcen und Links

### Dokumentation
- [OpenAI Whisper GitHub](https://github.com/openai/whisper)
- [KeyBERT Documentation](https://maartengr.github.io/KeyBERT/)
- [spaCy German Models](https://spacy.io/models/de)

### Testdaten
- Freie Vorlesungsvideos: MIT OpenCourseWare, Khan Academy
- Deutsche Universit√§ten: TU M√ºnchen, RWTH Aachen (√∂ffentliche Lectures)

### Tools
- **Audio-Editing:** Audacity, ffmpeg
- **Evaluation:** WER-Calculation Tools
- **Deployment:** Docker, Flask, nginx

---

*Letzte Aktualisierung: [Datum]*  
*Status: In Entwicklung*